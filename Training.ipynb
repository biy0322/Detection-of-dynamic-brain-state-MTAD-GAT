{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/camin1/inyoung/DATA/data_pre\")\n",
    "data_list = os.listdir()\n",
    "os.chdir(\"/camin1/inyoung/MTAD-GAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.loss import JointLoss\n",
    "from model.mtad_gat import MTAD_GAT\n",
    "from utils.adjustpred import adjust_predicts\n",
    "from utils.earlystop import EarlyStop\n",
    "from utils.evalmethods import pot_threshold, epsilon_threshold, bestf1_threshold\n",
    "from utils.plot import plot_loss\n",
    "from utils.preprocess import preprocess\n",
    "from utils.setseed import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, w=64):\n",
    "        self.data = data\n",
    "        self.w = w\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index + self.w]\n",
    "        y = self.data[index + self.w:index + self.w + 1]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp:\n",
    "    def __init__(self, iter, name, epochs, batch_size, patience, lr, generate, train_x, valid_x, test_x,  w=64, gamma=1):\n",
    "        ## hyper-parameter\n",
    "        self.iter = iter\n",
    "        self.name = name\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.w = w\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.check_point = '/camin1/inyoung/MTAD-GAT/checkpoint/'+ name + '_chechkpoint_iter_'+  \"_\" + str(iter)+',pkl'\n",
    "        \n",
    "        self.train_x = train_x.values\n",
    "        self.valid_x = valid_x.values\n",
    "        self.test_x = test_x.values\n",
    "        \n",
    "        self._get_data()\n",
    "        self._get_model()\n",
    "    \n",
    "    ################################################ Make Dataset ################################################\n",
    "    def _get_data(self, train=True):\n",
    "        if train:\n",
    "            trainset = MyDataset(self.train_x, w=self.w)\n",
    "            validset = MyDataset(self.valid_x, w=self.w)\n",
    "            testset = MyDataset(self.test_x, w=self.w)\n",
    "            \n",
    "            self.trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            self.validloader = DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
    "            self.testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            self.loss = {'train': {'forecast': [], 'reconstruct': [], 'total': []},\n",
    "                         'valid': {'forecast': [], 'reconstruct': [], 'total': []}}\n",
    "\n",
    "            print('train: {0}, valid: {1}, test: {2}'.format(len(trainset), len(validset), len(testset)))\n",
    "        \n",
    "        else:\n",
    "            self.train_x = np.vstack((self.train_x, self.valid_x))\n",
    "            \n",
    "            trainset = MyDataset(self.train_x, w=self.w)\n",
    "            testset = MyDataset(self.test_x, w=self.w)\n",
    "                \n",
    "            self.trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            self.testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            print('train: {0}, test: {1}'.format(len(trainset), len(testset)))\n",
    "    \n",
    "    ################################################ Load Model ################################################\n",
    "    def _get_model(self):\n",
    "        self.model = MTAD_GAT().to(self.device)\n",
    "        self.criterion = JointLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        self.earlystopping = EarlyStop(patience=self.patience)\n",
    "        \n",
    "    ################################ Training Model for each batch ################################\n",
    "    def _process_one_batch(self, batch_x, batch_y):\n",
    "        batch_x = batch_x.float().to(self.device)\n",
    "        batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "        reconstruct, forecast = self.model(batch_x)\n",
    "        forecast_loss, reconstruct_loss, loss = self.criterion(batch_x, batch_y, reconstruct, forecast)\n",
    "\n",
    "        return forecast_loss, reconstruct_loss, loss\n",
    "    \n",
    "    ################################ 각 시점에 대한 이상치 점수 산출 ################################\n",
    "    def _get_score(self, data, dataloader):\n",
    "        self.model.eval()\n",
    "        forecasts, reconstructs = [], []\n",
    "        \n",
    "        for (batch_x, batch_y) in dataloader:\n",
    "            batch_x = batch_x.float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "            \n",
    "            _, forecast = self.model(batch_x)\n",
    "            recon_x = torch.cat((batch_x[:, 1:, :], batch_y), dim=1)\n",
    "            reconstruct,_ = self.model(recon_x)\n",
    "            \n",
    "            forecasts.append(forecast.detach().cpu().numpy())\n",
    "            reconstructs.append(reconstruct.detach().cpu().numpy()[:,-1,:])\n",
    "        \n",
    "        forecasts = np.concatenate(forecasts, axis=0).squeeze() \n",
    "        reconstructs = np.concatenate(reconstructs, axis=0)\n",
    "        actuals = data[self.w:]\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        scores = np.zeros_like(actuals)\n",
    "        \n",
    "        for i in range(actuals.shape[1]): ## 변수의 개수만큼!\n",
    "            df[\"For_\"+str(i)] = forecasts[:,i] ## 예측값\n",
    "            df[\"Rec_\"+str(i)] = reconstructs[:,i] ## 재건축한 값\n",
    "            df[\"Act_\"+str(i)] = actuals[:,i] ## 실제값\n",
    "            \n",
    "            score = np.sqrt((forecasts[:,i] - actuals[:,i])**2) + self.gamma*np.sqrt((reconstructs[:,i]-actuals[:,i])**2)\n",
    "            scores[:,i] = score\n",
    "            df[\"Score_\"+str(i)] = score ## 이상치 점수\n",
    "        \n",
    "        scores = np.mean(scores, axis=1) ## 각 행의 평균 -> 64개의 원소를 가진 1차원 배열 \n",
    "        df['Score_Global'] = scores ## 각 시점별 score_global 값 산출\n",
    "        return df\n",
    "    \n",
    "    def fit(self):\n",
    "        ################################ 학습하기 전 initial loss ################################\n",
    "        self.model.eval()\n",
    "        train_forecast_loss, train_reconstruct_loss, train_loss = [], [], []\n",
    "        \n",
    "        for (batch_x, batch_y) in tqdm(self.trainloader):\n",
    "            forecast_loss, reconstruct_loss, loss = self._process_one_batch(batch_x, batch_y)\n",
    "            train_forecast_loss.append(forecast_loss.item())\n",
    "            train_reconstruct_loss.append(reconstruct_loss.item())\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        self.model.eval()\n",
    "        valid_forecast_loss, valid_reconstruct_loss, valid_loss = [], [], []\n",
    "        for (batch_x, batch_y) in self.validloader:\n",
    "            forecast_loss, reconstruct_loss, loss = self._process_one_batch(batch_x, batch_y)\n",
    "            valid_forecast_loss.append(forecast_loss.item())\n",
    "            valid_reconstruct_loss.append(reconstruct_loss.item())\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "        train_forecast_loss = np.sqrt(np.average(np.array(train_forecast_loss) ** 2))\n",
    "        valid_forecast_loss = np.sqrt(np.average(np.array(valid_forecast_loss) ** 2))\n",
    "        train_reconstruct_loss = np.sqrt(np.average(np.array(train_reconstruct_loss) ** 2))\n",
    "        valid_reconstruct_loss = np.sqrt(np.average(np.array(valid_reconstruct_loss) ** 2))\n",
    "        train_loss = np.sqrt(np.average(np.array(train_loss) ** 2))\n",
    "        valid_loss = np.sqrt(np.average(np.array(valid_loss) ** 2))\n",
    "\n",
    "        print(\n",
    "            \"Iter: {0} Init || Total Loss| Train: {1:.6f} Vali: {2:.6f} || Forecast Loss| Train:{3:.6f} Valid\"\n",
    "            \": {4:.6f} || Reconstruct Loss| Train: {5:.6f} Valid: {6:.6f}\".format(\n",
    "                self.iter, train_loss, valid_loss, train_forecast_loss, valid_forecast_loss,\n",
    "                train_reconstruct_loss, valid_reconstruct_loss))\n",
    "    \n",
    "        ################################ Train ################################\n",
    "        for e in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_forecast_loss, train_reconstruct_loss, train_loss = [], [], []\n",
    "            for (batch_x, batch_y) in tqdm(self.trainloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                forecast_loss, reconstruct_loss, loss = self._process_one_batch(batch_x, batch_y)\n",
    "                train_forecast_loss.append(forecast_loss.item())\n",
    "                train_reconstruct_loss.append(reconstruct_loss.item())\n",
    "                train_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "            valid_forecast_loss, valid_reconstruct_loss, valid_loss = [], [], []\n",
    "            for (batch_x, batch_y) in self.validloader:\n",
    "                forecast_loss, reconstruct_loss, loss = self._process_one_batch(batch_x, batch_y)\n",
    "                valid_forecast_loss.append(forecast_loss.item())\n",
    "                valid_reconstruct_loss.append(reconstruct_loss.item())\n",
    "                valid_loss.append(loss.item())\n",
    "\n",
    "            train_forecast_loss = np.sqrt(np.average(np.array(train_forecast_loss) ** 2))\n",
    "            valid_forecast_loss = np.sqrt(np.average(np.array(valid_forecast_loss) ** 2))\n",
    "            train_reconstruct_loss = np.sqrt(np.average(np.array(train_reconstruct_loss) ** 2))\n",
    "            valid_reconstruct_loss = np.sqrt(np.average(np.array(valid_reconstruct_loss) ** 2))\n",
    "            train_loss = np.sqrt(np.average(np.array(train_loss) ** 2))\n",
    "            valid_loss = np.sqrt(np.average(np.array(valid_loss) ** 2))\n",
    "\n",
    "            self.loss['train']['forecast'].append(train_forecast_loss)\n",
    "            self.loss['train']['reconstruct'].append(train_reconstruct_loss)\n",
    "            self.loss['train']['total'].append(train_loss)\n",
    "            self.loss['valid']['forecast'].append(valid_forecast_loss)\n",
    "            self.loss['valid']['reconstruct'].append(valid_reconstruct_loss)\n",
    "            self.loss['valid']['total'].append(valid_loss)\n",
    "\n",
    "            print(\n",
    "                \"Iter: {0} Epoch: {1} || Total Loss| Train: {2:.6f} Vali: {3:.6f} || Forecast Loss| Train:{4:.6f} Valid\"\n",
    "                 \": {5:.6f} || Reconstruct Loss| Train: {6:.6f} Valid: {7:.6f}\".format(\n",
    "                     self.iter, e+1, train_loss, valid_loss, train_forecast_loss, valid_forecast_loss,\n",
    "                    train_reconstruct_loss, valid_reconstruct_loss))\n",
    "\n",
    "            self.earlystopping(valid_loss, self.model, self.check_point)\n",
    "            if self.earlystopping.early_stop:\n",
    "                print(\"Iter {0} is Early stopping!\".format(self.iter))\n",
    "                break\n",
    "            \n",
    "        self.model.load_state_dict(torch.load(self.check_point))\n",
    "\n",
    "        plot_loss(self.loss[\"train\"][\"forecast\"], self.loss[\"train\"][\"reconstruct\"], self.loss[\"train\"][\"total\"],\n",
    "                 '/camin1/inyoung/MTAD-GAT/img/' + '_iter' + self.name + \"_\" + str(self.iter) + '_trainloss.png')\n",
    "        plot_loss(self.loss[\"valid\"][\"forecast\"], self.loss[\"valid\"][\"reconstruct\"], self.loss[\"valid\"][\"total\"],\n",
    "                '/camin1/inyoung/MTAD-GAT/img/' + '_iter' + self.name + \"_\" + str(self.iter) + str(self.iter) + '_validloss.png')\n",
    "    \n",
    "    ################################ Predict for test dataset ################################\n",
    "    def predict(self, model_load=False, data_load=False):\n",
    "        if model_load:\n",
    "            self.model.load_state_dict(torch.load(self.check_point))\n",
    "        self._get_data(train=False)\n",
    "        \n",
    "        \n",
    "        # actual_label = self.test_y[self.w:]\n",
    "        \n",
    "        if data_load:\n",
    "            trainresult = pd.read_csv('/camin1/inyoung/MTAD-GAT/result/'  + '_iter' + str(self.iter) + '_trainresult.csv')\n",
    "            testresult = pd.read_csv('/camin1/inyoung/MTAD-GAT/result/' + '_iter' + str(self.iter) + '_testresult.csv')\n",
    "        else:\n",
    "            trainresult = self._get_score(self.train_x, self.trainloader)\n",
    "            testresult = self._get_score(self.test_x, self.testloader)\n",
    "            \n",
    "        for i in range(self.test_x.shape[1]):\n",
    "            train_score = trainresult[\"Score_\" + str(i)].values\n",
    "            test_score = testresult[\"Score_\" + str(i)].values\n",
    "\n",
    "            threshold = pot_threshold(train_score, test_score)\n",
    "\n",
    "            train_pred = (train_score > threshold).astype(np.int64) ## 1 또는 0\n",
    "            test_pred = (test_score > threshold).astype(np.int64)\n",
    "\n",
    "            trainresult[\"Pred_\" + str(i)] = train_pred\n",
    "            trainresult[\"Threshold_\" + str(i)] = threshold\n",
    "            testresult[\"Pred_\" + str(i)] = test_pred\n",
    "            testresult[\"Threshold_\" + str(i)] = threshold\n",
    "\n",
    "        train_score = trainresult[\"Score_Global\"].values\n",
    "        test_score = testresult[\"Score_Global\"].values\n",
    "\n",
    "        #threshold = pot_threshold(train_score, test_score)\n",
    "        # threshold = bestf1_threshold(test_score, actual_label)\n",
    "        threshold = epsilon_threshold(train_score)\n",
    "\n",
    "        train_pred = (train_score > threshold).astype(np.int64)\n",
    "        test_pred = (test_score > threshold).astype(np.int64)\n",
    "        \n",
    "        trainresult[\"Pred_Global\"] = train_pred\n",
    "        #trainresult[\"Label_Global\"] = 0\n",
    "        trainresult[\"Threshold_Global\"] = threshold\n",
    "\n",
    "        testresult[\"Pred_Global\"] = test_pred\n",
    "        #testresult[\"Label_Global\"] = actual_label\n",
    "        testresult[\"Threshold_Global\"] = threshold\n",
    "        \n",
    "        trainresult.to_csv('/camin1/inyoung/MTAD-GAT/result/' + self.name + '_iter' +  \"_\" +  str(self.iter) + '_trainresult.csv', index=False)\n",
    "        testresult.to_csv('/camin1/inyoung/MTAD-GAT/result/'  + self.name + '_iter' + \"_\" + str(self.iter) + '_testresult.csv', index=False)\n",
    "\n",
    "        #print(\"Iter {0} || precision: {2:.6f} recall: {3:.6f} f1: {4:.6f}\".format(\n",
    "        #    self.iter, precision_score(actual_label, test_pred),\n",
    "        #    recall_score(actual_label, test_pred), f1_score(actual_label, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/camin1/inyoung/DATA/data_pre/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = [i for i in data_list if i.split(\"_\")[0]==\"train\"]\n",
    "test_data_list = [i for i in data_list if i.split(\"_\")[0]==\"test\"]\n",
    "name_list = [i.split(\"_\")[1][:-4] for i in train_data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n",
      "1010\n",
      "1010\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_list))\n",
    "print(len(test_data_list))\n",
    "print(len(name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters=1\n",
    "epochs=50\n",
    "batch_size=16\n",
    "patience =3\n",
    "lr=0.01\n",
    "generate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_data_list)):\n",
    "    print(\"###########################################\" + name_list[i] + \"-th dataset###########################################\")\n",
    "    train_x = pd.read_csv(path+train_data_list[i], header=0)\n",
    "    test_x = pd.read_csv(path+test_data_list[i], header=0)\n",
    "\n",
    "    val_split = 0.3\n",
    "    valid_x = train_x.iloc[-int(val_split * len(train_x)):,:]\n",
    "    train_x = train_x.iloc[:-int(val_split * len(train_x)),:]\n",
    "    \n",
    "    for it in range(iters):\n",
    "        print(\"iter \" + str(it) + ' is start...')\n",
    "        exp = Exp(it, name_list[i], epochs, batch_size, patience, lr, generate, train_x, valid_x, test_x,  w=64, gamma=1)\n",
    "        exp.fit()\n",
    "        exp.predict(model_load=False, data_load=False)\n",
    "        print(\"iter \" + str(it) + ' is end!')\n",
    "    ################### empty gpu cash ###################    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
